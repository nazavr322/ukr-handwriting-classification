{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f90fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nazar/Projects/ukrainian_handwriting\n",
      "/home/nazar/Projects/ukrainian_handwriting/notebooks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%cd ../\n",
    "from src.data.datasets import HandwritingDataset\n",
    "from src.models.models import HandwritingClassifier\n",
    "%cd notebooks/\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0c9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bac53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = HandwritingClassifier._mean\n",
    "STD = HandwritingClassifier._std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240d8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = T.Compose([\n",
    "    T.RandomRotation(30),\n",
    "    T.RandomAffine(0, (0.1, 0.1)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=MEAN, std=STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ed963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training data: 1281\n",
      "Number of samples in test data: 300\n"
     ]
    }
   ],
   "source": [
    "train_data = HandwritingDataset(\n",
    "    '../data/processed/train_data.csv',\n",
    "    transforms=tf\n",
    ")\n",
    "\n",
    "test_data = HandwritingDataset(\n",
    "    '../data/processed/test_data.csv',\n",
    "    transforms=T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    ")\n",
    "\n",
    "print('Number of samples in training data:', len(train_data))\n",
    "print('Number of samples in test data:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddfadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VAL_SIZE = 100\n",
    "\n",
    "indices = list(range(len(train_data)))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[VAL_SIZE:], indices[:VAL_SIZE]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_data, BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(train_data, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2751baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(params: dict):\n",
    "    model = HandwritingClassifier()\n",
    "    model.load_state_dict(torch.load('../models/mnist_model.pt'), strict=False)\n",
    "    \n",
    "    model.type(torch.cuda.FloatTensor)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion_1 = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "    criterion_2 = nn.BCEWithLogitsLoss().type(torch.cuda.FloatTensor)\n",
    "    losses = (criterion_1, criterion_2)\n",
    "    \n",
    "    LR = params['learning_rate']\n",
    "    REG = params['weight_decay']\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, weight_decay=REG)\n",
    "    \n",
    "    factor = params['factor']\n",
    "    patience = params['patience']\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=factor,\n",
    "                                                     patience=patience)\n",
    "    return model, losses, optimizer, scheduler\n",
    "\n",
    "\n",
    "def compute_accuracy(prediction, ground_truth):\n",
    "    correct = torch.sum(prediction == ground_truth).item()\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "\n",
    "def validate(model, losses, loader):\n",
    "    model.eval()\n",
    "    lbl_acc = 0\n",
    "    is_upp_acc = 0\n",
    "    loss_acum = 0\n",
    "    for i, (x, *y, _) in enumerate(loader):\n",
    "        x_gpu = x.to(device)\n",
    "        y[1] = y[1].unsqueeze(1).float()\n",
    "        y_gpu = tuple(targ.to(device) for targ in y)\n",
    "        \n",
    "        prediction = model(x_gpu)\n",
    "        loss_value = sum(\n",
    "            loss(out, targ) for loss, out, targ in zip(losses, prediction, y_gpu)\n",
    "        )\n",
    "        \n",
    "        loss_acum += loss_value.item()\n",
    "        lbl = torch.argmax(prediction[0], 1)\n",
    "        lbl_acc += compute_accuracy(lbl, y_gpu[0])\n",
    "        is_upp = 0 if prediction[1].item() < 0.5 else 1\n",
    "        is_upp_acc += compute_accuracy(is_upp, y_gpu[1])\n",
    "    return loss_acum / i, lbl_acc / i, is_upp_acc / i\n",
    "\n",
    "def train_model(params: dict):\n",
    "    model, losses, optimizer, scheduler = build_model(params)\n",
    "    \n",
    "    num_epochs = params['num_epochs']\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for i, (x, *y, _) in enumerate(train_loader):\n",
    "            x_gpu = x.to(device)\n",
    "            y[1] = y[1].unsqueeze(1).float()\n",
    "            y_gpu = tuple(target.to(device) for target in y)\n",
    "            \n",
    "            prediction = model(x_gpu)\n",
    "            loss_value = sum(\n",
    "                loss(out, targ) for loss, out, targ in zip(losses, prediction, y_gpu)\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_loss, lbl_acc, is_upp_acc = validate(model, losses, val_loader)\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "    return lbl_acc, is_upp_acc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 30, 30),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ('SGD',)),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 7e-3, 7e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-3, 7e-3, log=True),\n",
    "        'scheduler': trial.suggest_categorical('scheduler', ('ReduceLROnPlateau',)),\n",
    "        'factor': trial.suggest_float('factor', 0.05, 0.2),\n",
    "        'patience': trial.suggest_int('patience', 2, 4),\n",
    "    }\n",
    "    \n",
    "    lbl_acc, is_upp_acc = train_model(params)\n",
    "    \n",
    "    return lbl_acc, is_upp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "477a4fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-24 16:25:40,036]\u001b[0m A new study created in memory with name: Test run for mnist + glyphs and case determination\u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:26:30,282]\u001b[0m Trial 0 finished with values: [0.8181818181818182, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.021566075664290255, 'weight_decay': 0.0014545889488107417, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1852627813476765, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:27:19,003]\u001b[0m Trial 1 finished with values: [0.797979797979798, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02370433346337466, 'weight_decay': 0.0038705402738629493, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.0867850350584129, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:28:15,869]\u001b[0m Trial 2 finished with values: [0.7676767676767676, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.059584195093264083, 'weight_decay': 0.0020492035515507637, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11343743747883442, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:29:07,736]\u001b[0m Trial 3 finished with values: [0.7171717171717171, 0.9090909090909091] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.008785950981658299, 'weight_decay': 0.0010784591826553665, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.10668179399577767, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:29:58,063]\u001b[0m Trial 4 finished with values: [0.7676767676767676, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.016431612402160338, 'weight_decay': 0.0029747546131584485, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.10233746341915449, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:30:49,044]\u001b[0m Trial 5 finished with values: [0.7676767676767676, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.016247261233466388, 'weight_decay': 0.003425573552133891, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.12436000914426751, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:31:41,730]\u001b[0m Trial 6 finished with values: [0.7474747474747475, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.06741715982940119, 'weight_decay': 0.0035591551653813523, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.07256131752278501, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:32:35,755]\u001b[0m Trial 7 finished with values: [0.7777777777777778, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02713007190494038, 'weight_decay': 0.005022256879933254, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14730039197922423, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:33:29,209]\u001b[0m Trial 8 finished with values: [0.797979797979798, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.020526859702698033, 'weight_decay': 0.0023990908420711246, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14192713992609246, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:34:21,160]\u001b[0m Trial 9 finished with values: [0.7272727272727273, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.01118623916337048, 'weight_decay': 0.0011782397742675231, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.06537617164643585, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:35:10,636]\u001b[0m Trial 10 finished with values: [0.8383838383838383, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.012947555925250388, 'weight_decay': 0.0012874435583927798, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14223426672659512, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:35:58,998]\u001b[0m Trial 11 finished with values: [0.7575757575757576, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.01110617975903588, 'weight_decay': 0.003044373219298379, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11154433224150342, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:36:46,810]\u001b[0m Trial 12 finished with values: [0.797979797979798, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.0487531028255518, 'weight_decay': 0.006039900464247551, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1775058455824457, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:37:35,579]\u001b[0m Trial 13 finished with values: [0.8282828282828283, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.016956319531990123, 'weight_decay': 0.004525658002525949, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.19030743729243638, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:38:25,526]\u001b[0m Trial 14 finished with values: [0.7878787878787878, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.019195621723933978, 'weight_decay': 0.002251052779958408, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.17681602014501063, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:39:21,620]\u001b[0m Trial 15 finished with values: [0.7575757575757576, 0.9191919191919192] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.00782881172830229, 'weight_decay': 0.0010396567914578525, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1601906790904548, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:40:22,497]\u001b[0m Trial 16 finished with values: [0.7575757575757576, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.007849787611358813, 'weight_decay': 0.001186264826311976, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.06249745435839963, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:41:14,190]\u001b[0m Trial 17 finished with values: [0.8080808080808081, 0.9797979797979798] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.03203931610054596, 'weight_decay': 0.0013535825473579746, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.13358014594178225, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:42:02,708]\u001b[0m Trial 18 finished with values: [0.797979797979798, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02002019341758859, 'weight_decay': 0.005539633436002512, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1889799701353359, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:42:51,672]\u001b[0m Trial 19 finished with values: [0.7676767676767676, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.008815524841504135, 'weight_decay': 0.004098166194615442, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.07851274238416393, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:43:39,277]\u001b[0m Trial 20 finished with values: [0.8686868686868687, 0.9191919191919192] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.013590597260231593, 'weight_decay': 0.001962195816333336, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14087683949161897, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:44:28,081]\u001b[0m Trial 21 finished with values: [0.7474747474747475, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.04191120544591644, 'weight_decay': 0.0029104227084499816, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11720612614183175, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:45:16,978]\u001b[0m Trial 22 finished with values: [0.7878787878787878, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.009733490596064572, 'weight_decay': 0.001212978600460886, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.07737942206785961, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:46:04,107]\u001b[0m Trial 23 finished with values: [0.8181818181818182, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02487179153777384, 'weight_decay': 0.0010221815801702478, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.17557322989951335, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:46:51,149]\u001b[0m Trial 24 finished with values: [0.8181818181818182, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.0234762599760526, 'weight_decay': 0.0051369884340011, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11622926260580123, 'patience': 3}. \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-24 16:47:38,181]\u001b[0m Trial 25 finished with values: [0.7575757575757576, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.013390240964420944, 'weight_decay': 0.006106049854157119, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.12825262997485454, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:48:25,358]\u001b[0m Trial 26 finished with values: [0.7575757575757576, 0.9090909090909091] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.01769472992744681, 'weight_decay': 0.002246485698365526, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.0766994355634878, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:49:12,797]\u001b[0m Trial 27 finished with values: [0.7272727272727273, 0.9090909090909091] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.009973956071770818, 'weight_decay': 0.004141540449724739, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.19972520361979107, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:50:00,072]\u001b[0m Trial 28 finished with values: [0.7676767676767676, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.04116261925681394, 'weight_decay': 0.005880474173964196, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1258560904606828, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:50:47,114]\u001b[0m Trial 29 finished with values: [0.7272727272727273, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.01436169503303966, 'weight_decay': 0.004179280129717217, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.0804030971185154, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:51:34,303]\u001b[0m Trial 30 finished with values: [0.7575757575757576, 0.9797979797979798] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.014779042645196519, 'weight_decay': 0.0024330286344997603, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.11750840172117663, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:52:21,406]\u001b[0m Trial 31 finished with values: [0.8080808080808081, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.03086998779478926, 'weight_decay': 0.002602809994869992, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.17372280552224129, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:53:08,756]\u001b[0m Trial 32 finished with values: [0.7676767676767676, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.013841283683389037, 'weight_decay': 0.004899034218250789, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14015832212059737, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:53:59,705]\u001b[0m Trial 33 finished with values: [0.7676767676767676, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.00960446611855239, 'weight_decay': 0.00338255665274593, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.15398089455934266, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:55:22,766]\u001b[0m Trial 34 finished with values: [0.7373737373737373, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.015256634838148556, 'weight_decay': 0.001598013497765095, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.14407825491630355, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:56:41,229]\u001b[0m Trial 35 finished with values: [0.7272727272727273, 0.9090909090909091] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.013244896992341574, 'weight_decay': 0.005337953127604569, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.09650114223229692, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:57:59,343]\u001b[0m Trial 36 finished with values: [0.7676767676767676, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.031475359577668616, 'weight_decay': 0.0031077351885869697, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.08187573462259085, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 16:59:18,007]\u001b[0m Trial 37 finished with values: [0.7272727272727273, 0.9696969696969697] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.035473038255014154, 'weight_decay': 0.001870672890881185, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.057289121865613746, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:00:36,245]\u001b[0m Trial 38 finished with values: [0.7272727272727273, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02737836499589383, 'weight_decay': 0.006400432171306656, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.15659564982878477, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:01:54,603]\u001b[0m Trial 39 finished with values: [0.7878787878787878, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.03197562722062043, 'weight_decay': 0.0029957913869364974, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1792229028427597, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:03:13,905]\u001b[0m Trial 40 finished with values: [0.696969696969697, 0.9292929292929293] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.04153506526489461, 'weight_decay': 0.0011063630382306207, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1358869938589496, 'patience': 2}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:04:32,172]\u001b[0m Trial 41 finished with values: [0.7676767676767676, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.014087958612293776, 'weight_decay': 0.0025937124322258924, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.09885289247107534, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:05:58,766]\u001b[0m Trial 42 finished with values: [0.7474747474747475, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.036528714194223114, 'weight_decay': 0.00430728128087797, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.09901424931465633, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:07:29,013]\u001b[0m Trial 43 finished with values: [0.7373737373737373, 0.9393939393939394] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.030847391651663243, 'weight_decay': 0.0012804443789530204, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.08805828564649107, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:08:50,195]\u001b[0m Trial 44 finished with values: [0.8484848484848485, 0.9494949494949495] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.02260243436826406, 'weight_decay': 0.0014668035410638639, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.05609252468461898, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:09:48,509]\u001b[0m Trial 45 finished with values: [0.7777777777777778, 0.9797979797979798] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.03287731503598747, 'weight_decay': 0.0018670323116051297, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.09319173618460852, 'patience': 4}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:10:41,433]\u001b[0m Trial 46 finished with values: [0.7070707070707071, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.03820986329675782, 'weight_decay': 0.0068900384677578605, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.06627139533666088, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:11:34,800]\u001b[0m Trial 47 finished with values: [0.7676767676767676, 0.9191919191919192] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.008980191227020009, 'weight_decay': 0.0013769302124265628, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.05127361322603988, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:12:33,669]\u001b[0m Trial 48 finished with values: [0.8282828282828283, 0.9595959595959596] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.030913564972544008, 'weight_decay': 0.0012892639127623099, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.18157831708090771, 'patience': 3}. \u001b[0m\n",
      "\u001b[32m[I 2022-07-24 17:13:54,760]\u001b[0m Trial 49 finished with values: [0.6666666666666666, 0.8787878787878788] and parameters: {'num_epochs': 30, 'optimizer': 'SGD', 'learning_rate': 0.008207608165268455, 'weight_decay': 0.0015989485227734892, 'scheduler': 'ReduceLROnPlateau', 'factor': 0.1089120303295988, 'patience': 4}. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "name = 'Test run for mnist + glyphs and case determination'\n",
    "study = optuna.create_study(study_name=name, directions=['maximize', 'maximize'])\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe3c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trials[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c730275",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
